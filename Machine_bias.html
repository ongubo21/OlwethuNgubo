<html>
<head>
  <style>
  body {
    background-image: url("https://qph.fs.quoracdn.net/main-qimg-fa8c8706fcacec1fa30dc127ebbfe984-c ");
    background-position: center;
    background-size: contain;
  }
  .bold{
    font-family: cursive;
    color: rgb(68, 60, 227);

  }
  .all{
    border-bottom:120px;
    border-top:120px;
    text-align: center;
    width: 60%;
    margin: 0 auto;
    font-family: sans-serif;
    padding-left: 120px;
    padding-right: 120px;

    line-height: inherit;
    font-size: 150%;
    /* background: url(https://goo.gl/images/UOim6l); */
    background-color:rgba(66, 219, 240, .5)


  }
  </style>
</head>
<body>
  <h1 class= "bold"><strong> Machine Bias Reflection by Olwethu Ngubo</strong></h1>
  <div class="all">
    <p>Machine bias is a software used by courts across several states in the USA to predict  the likelihood of criminals commiting more crimes in the future.
  The risk assessments made by these computers  are believed to be biased against people of color.Black people are more likely  to be labeled as high risks,
   for example a black man who has commited petty theft but has no prior offense or criminal record is going to labeled a high-risk as opposed to a white man
    who has a very dirty record and has been charged for shoplifting being labeled a low-risk. All that means is that the black man who has never done any criminal
  activity  before his petty theft will be more likely to commit crime in the future than a white man who had committed multiple crimes prior to his
   previous one. There is more on how the computer works by <a href="https://www.propublica.org/article/breaking-the-black-box-how-machines-learn-to-be-racist?word=Clinton"> Clinton </a>
 </p>
  </div>

  <div class="all"><p>Several states across the USA give judges these risk assessment scores when they are making decisions about who they are setting free. Research has been done and
   they found that only 20% of the people predicted to commit crimes by this machine actually do in the future.

  The formula used by this machine is most likely to falsely flag black people as high risk rather than low and it mislabels white people as low risk rather than high
  risk.
   <a href="https://youtu.be/Ts35luE59d0"> Julia Angwin </a> explains it better. The results given by this machine are not even determined by factors such as previous
   crimes or types of crimes they are arrested for, but they are determined by factors such as whether or not the person has a job or the level of education the person
   has received and more.</p>
  </div>

 <div class="all"><p>The machine is not an accurate way of determining future criminals. I do not think that there is a way of accurately predicting future criminals because human beings
  are individuals and they can not be examined as a whole or generalized with the majority.

On the contrary I get where they are from because as the article mentioned, for centuries the key to decision-making in courts relied on human beings who can be
biased and are guided by their personal instincts, prejudices and stereotypes instilled on them by society.

They probably had a vision for that machine, which is handing a portion of the justice system over to a completely neutral medium. If the computer could predict
accurately who was likely to commit more crimes in future then the justice system would be fairer, the trick is to point the computer in the right direction so it
gets it right.</p></div>

</body>
</html>
